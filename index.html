<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Srinadh Bhojanapalli</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Info</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Srinadh Bhojanapalli</h1>
</div>
<table class="imgtable"><tr><td>
<img src="img/profile.jpg" alt="srinadh" width="200px" height="231px" />&nbsp;</td>
<td align="left"><p>I am a senior research scientist at Google research in New York. <br /></p>
<p>Earlier I was a research assistant professor at <a href="http://www.ttic.edu" target=&ldquo;blank&rdquo;>TTI Chicago</a>. I obtained my PhD in ECE at The University of Texas at Austin where I was advised by <a href="http://users.ece.utexas.edu/~sanghavi/" target=&ldquo;blank&rdquo;>Prof. Sujay Sanghavi</a>. Before coming to UT, I've spent four wonderful years pursuing my undergraduate studies at Indian Institute of Technology Bombay.</p>
<p><b>Contact:</b> bsrinadh [at] google [dot] com.</p>
</td></tr></table>
<h2>News</h2>
<ul>
<li><p>New Preprints on biases in Distillation and low dimensionality of Attention in Transformers.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2106.10494" target=&ldquo;blank&rdquo;>Teacher's pet: understanding and mitigating biases in distillation</a>.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2106.08823" target=&ldquo;blank&rdquo;>Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation</a>.</p>
</li></ul>
</li>
<li><p>Papers accepted to ICCV and EMNLP 2021 on</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2104.08698" target=&ldquo;blank&rdquo;>Better position and segment encodings for Transformers</a>.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2103.14586" target=&ldquo;blank&rdquo;>Understanding robustness of Transformers for image classification tasks (ViT models)</a>.</p>
</li></ul>
</li>
<li><p>ICLR 2021: One paper accepted on <a href="https://openreview.net/forum?id=BtZhsSGNRNi" target=&ldquo;blank&rdquo;>Coping with Label Shift via Distributionally Robust Optimisation</a>. </p>
</li>
<li><p>Neurips 2020: Two papers accepted on universal approximability of Sparse Transformers and efficient formulations for stagewise optimization problems.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.04862" target=&ldquo;blank&rdquo;>O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers</a>.</p>
</li>
<li><p><a href="https://arxiv.org/abs/2010.14322" target=&ldquo;blank&rdquo;>An efficient nonconvex reformulation of stagewise convex optimization problems</a>.</p>
</li></ul>
</li>
<li><p>EMNLP 2020: One paper accepted on semantic label smoothing.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/2010.07447" target=&ldquo;blank&rdquo;>Semantic label smoothing for sequence to sequence problems</a>.</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-09-13 11:53:43 EDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
